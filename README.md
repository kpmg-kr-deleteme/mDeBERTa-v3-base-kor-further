# mDeBERTa-v3-base-kor-further
Further pre-trained mDeBERTa-v3-base Model with Korean dataset

> ğŸ’¡ ì•„ë˜ í”„ë¡œì íŠ¸ëŠ”Â KPMG Lighthouse Koreaì—ì„œ ì§„í–‰í•˜ì˜€ìŠµë‹ˆë‹¤.   
> KPMG Lighthouse Koreaì—ì„œëŠ”, Financial areaì˜ ë‹¤ì–‘í•œ ë¬¸ì œë“¤ì„ í•´ê²°í•˜ê¸° ìœ„í•´ Edge Technologyì˜ NLP/Vision AIë¥¼ ëª¨ë¸ë§í•˜ê³  ìˆìŠµë‹ˆë‹¤.
  

## What is DeBERTa?
- [DeBERTa](https://arxiv.org/abs/2006.03654)ëŠ” `Disentangled Attention` + `Enhanced Mask Decoder` ë¥¼ ì ìš©í•˜ì—¬ ë‹¨ì–´ì˜ positional informationì„ íš¨ê³¼ì ìœ¼ë¡œ í•™ìŠµí•©ë‹ˆë‹¤. ì´ì™€ ê°™ì€ ì•„ì´ë””ì–´ë¥¼ í†µí•´, ê¸°ì¡´ì˜ BERT, RoBERTaì—ì„œ ì‚¬ìš©í–ˆë˜ absolute position embeddingê³¼ëŠ” ë‹¬ë¦¬ DeBERTaëŠ” ë‹¨ì–´ì˜ ìƒëŒ€ì ì¸ ìœ„ì¹˜ ì •ë³´ë¥¼ í•™ìŠµ ê°€ëŠ¥í•œ ë²¡í„°ë¡œ í‘œí˜„í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµí•˜ê²Œ ë©ë‹ˆë‹¤. ê²°ê³¼ì ìœ¼ë¡œ, BERT, RoBERTA ì™€ ë¹„êµí–ˆì„ ë•Œ ë” ì¤€ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤.
- [DeBERTa-v3](https://arxiv.org/abs/2111.09543)ì—ì„œëŠ”, ì´ì „ ë²„ì „ì—ì„œ ì‚¬ìš©í–ˆë˜ MLM (Masked Language Model) ì„ RTD (Replaced Token Detection) Task ë¡œ ëŒ€ì²´í•œ ELECTRA ìŠ¤íƒ€ì¼ì˜ ì‚¬ì „í•™ìŠµ ë°©ë²•ê³¼, Gradient-Disentangled Embedding Sharing ì„ ì ìš©í•˜ì—¬ ëª¨ë¸ í•™ìŠµì˜ íš¨ìœ¨ì„±ì„ ê°œì„ í•˜ì˜€ìŠµë‹ˆë‹¤.
- DeBERTaì˜ ì•„í‚¤í…ì²˜ë¡œ í’ë¶€í•œ í•œêµ­ì–´ ë°ì´í„°ë¥¼ í•™ìŠµí•˜ê¸° ìœ„í•´ì„œ,  `mDeBERTa-v3-base-kor-further` ëŠ” microsoft ê°€ ë°œí‘œí•œ `mDeBERTa-v3-base` ë¥¼ ì•½ 40GBì˜ í•œêµ­ì–´ ë°ì´í„°ì— ëŒ€í•´ì„œ **ì¶”ê°€ì ì¸ ì‚¬ì „í•™ìŠµ**ì„ ì§„í–‰í•œ ì–¸ì–´ ëª¨ë¸ì…ë‹ˆë‹¤.
  
## How to Use
- Requirements
    ```
    pip install transformers
    pip install sentencepiece
    ```   
- [Huggingface Hub](https://huggingface.co/lighthouse/mdeberta-v3-base-kor-further)
    ```python
    from transformers import AutoModel, AutoTokenizer
    
    model = AutoModel.from_pretrained("mdeberta-v3-base-kor-further")  # DebertaV2ForModel
    tokenizer = AutoTokenizer.from_pretrained("mdeberta-v3-base-kor-further")  # DebertaV2Tokenizer (SentencePiece)
    ```

## Pre-trained Models
- ëª¨ë¸ì˜ ì•„í‚¤í…ì²˜ëŠ” ê¸°ì¡´ microsoftì—ì„œ ë°œí‘œí•œ `mdeberta-v3-base`ì™€ ë™ì¼í•œ êµ¬ì¡°ì…ë‹ˆë‹¤.
    
    |  | Vocabulary(K) | Backbone Parameters(M) | Hidden Size | Layers | Note |
    | --- | --- | --- | --- | --- | --- |
    | mdeberta-v3-base-kor-further (mdeberta-v3-baseì™€ ë™ì¼) | 250 | 86 | 768 | 12 | 250K new SPM vocab |

## Further Pretraing Details (MLM Task)
- `mDeBERTa-v3-base-kor-further` ëŠ” `microsoft/mDeBERTa-v3-base` ë¥¼ ì•½ 40GBì˜ í•œêµ­ì–´ ë°ì´í„°ì— ëŒ€í•´ì„œ MLM Taskë¥¼ ì ìš©í•˜ì—¬ ì¶”ê°€ì ì¸ ì‚¬ì „ í•™ìŠµì„ ì§„í–‰í•˜ì˜€ìŠµë‹ˆë‹¤.
    
    |  | Max length | Learning Rate | Batch Size | Train Steps | Warm-up Steps |
    | --- | --- | --- | --- | --- | --- |
    | mdeberta-v3-base-kor-further | 512 | 2e-5 | 8 | 5M | 50k |
    

## Datasets
- ëª¨ë‘ì˜ ë§ë­‰ì¹˜(ì‹ ë¬¸, êµ¬ì–´, ë¬¸ì–´), í•œêµ­ì–´ Wiki, êµ­ë¯¼ì²­ì› ë“± ì•½ 40 GB ì˜ í•œêµ­ì–´ ë°ì´í„°ì…‹ì´ ì¶”ê°€ì ì¸ ì‚¬ì „í•™ìŠµì— ì‚¬ìš©ë˜ì—ˆìŠµë‹ˆë‹¤.
    - Train: 10M lines, 5B tokens
    - Valid: 2M lines, 1B tokens
    - cf) ê¸°ì¡´ mDeBERTa-v3ì€ XLM-R ê³¼ ê°™ì´ [cc-100 ë°ì´í„°ì…‹](https://data.statmt.org/cc-100/)ìœ¼ë¡œ í•™ìŠµë˜ì—ˆìœ¼ë©°, ê·¸ ì¤‘ í•œêµ­ì–´ ë°ì´í„°ì…‹ì˜ í¬ê¸°ëŠ” 54GBì…ë‹ˆë‹¤.
    

## Fine-tuning on NLU Tasks - Base Model
| Model | Size | NSMC(acc) | Naver NER(F1) | PAWS (acc) | KorNLI (acc) | KorSTS (spearman) | Question Pair (acc) | KorQuaD (Dev) (EM/F1) | Korean-Hate-Speech (Dev) (F1) |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| XLM-Roberta-Base | 1.03G | 89.03 | 86.65 | 82.80 | 80.23 | 78.45 | 93.80 | 64.70 / 88.94 | 64.06 |
| mdeberta-base | 534M | 90.01 | 87.43 | 85.55 | 80.41 | **82.65** | 94.06 | 65.48 / 89.74 | 62.91 |
| mdeberta-base-kor-further (Ours) | 534M | **90.52** | **87.87** | **85.85** | **80.65** | 81.90 | **94.98** | **66.07 / 90.35** | **68.16** |

## Citation
```
@misc{he2021debertav3,
      title={DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing}, 
      author={Pengcheng He and Jianfeng Gao and Weizhu Chen},
      year={2021},
      eprint={2111.09543},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```

```
@inproceedings{
he2021deberta,
title={DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION},
author={Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=XPZIaotutsD}
}
```

## Reference
- [DeBERTa](https://github.com/microsoft/DeBERTa)
- [Huggingface Transformers](https://github.com/huggingface/transformers)
- [ëª¨ë‘ì˜ ë§ë­‰ì¹˜](https://corpus.korean.go.kr/)
- [Korpora: Korean Corpora Archives](https://github.com/ko-nlp/Korpora)
- [sooftware/Korean PLM](https://github.com/sooftware/Korean-PLM)
